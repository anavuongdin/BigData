{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigData-Spark-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iTcoLySMnVqv",
        "C06jhhJfa27F"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTcoLySMnVqv"
      },
      "source": [
        "# Setup Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VNQwCAinQbK"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5sUmLtxnvW9"
      },
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3P0zRT5n09t"
      },
      "source": [
        "!tar xf spark-3.0.3-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxdp-fmgpk6e"
      },
      "source": [
        "# Spark initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rl5SmQRnn32"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r8lMbUDlpnBY",
        "outputId": "a8114315-4747-4323-b36d-8b2c8b6f38f9"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.0.3-bin-hadoop2.7'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tuSMPXfp1j1"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "ZgkqZqr7p6KG",
        "outputId": "ebaee4ee-8e2e-437a-93b0-84165ba7c95c"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a49ca0d78720:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff0ec53bad0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C06jhhJfa27F"
      },
      "source": [
        "# JSON object classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkghckI_a7W4"
      },
      "source": [
        "class Date:\n",
        "  def __init__(self, date_pattern):\n",
        "    self.year, self.month, self.day = map(int, date_pattern.split(\"-\"))\n",
        "    assert self.year >= 0\n",
        "    assert 1 <= self.month <= 12\n",
        "    assert 1 <= self.day <= 31\n",
        "  \n",
        "  def __str__(self):\n",
        "    return \"{}-{}-{} 00:00:00\".format(self.day, self.month, self.year)\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    return (self.year, self.month, self.day) < (other.year, other.month, other.day)\n",
        "\n",
        "  def __le__(self, other):\n",
        "    return (self.year, self.month, self.day) <= (other.year, other.month, other.day)\n",
        "\n",
        "  def __gt__(self, other):\n",
        "    return (self.year, self.month, self.day) > (other.year, other.month, other.day)\n",
        "\n",
        "  def __ge__(self, other):\n",
        "    return (self.year, self.month, self.day) >= (other.year, other.month, other.day)\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return (self.year, self.month, self.day) == (other.year, other.month, other.day)\n",
        "\n",
        "  def __ne__(self, other):\n",
        "    return (self.year, self.month, self.day) != (other.year, other.month, other.day)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqdHmrE-a93e"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class DailyStock:\n",
        "  def __init__(self, symbol, date, open=-1, high=-1, low=-1, close=-1, adjusted_close=-1, volume=-1, dividend_amount=-1, split_coefficient=-1):\n",
        "    self.symbol = symbol\n",
        "    self.date = Date(date)\n",
        "    self.open = float(open)\n",
        "    self.high = float(high)\n",
        "    self.low = float(low)\n",
        "    self.close = float(close)\n",
        "    self.adjusted_close = float(adjusted_close)\n",
        "    self.volume = float(volume)\n",
        "    self.dividend_amount = float(dividend_amount)\n",
        "    self.split_coefficient = float(split_coefficient)\n",
        "  \n",
        "  def __str__(self):\n",
        "    return \"{}, {}, {}, {}, {}, {}, {}, {}, {}\".format(self.symbol, self.date, self.open, self.high, self.low, self.close, self.adjusted_close, self.dividend_amount, self.split_coefficient)\n",
        "  \n",
        "  def __lt__(self, other):\n",
        "    assert self.symbol == other.symbol\n",
        "    return self.date < other.date\n",
        "\n",
        "  def __le__(self, other):\n",
        "    assert self.symbol == other.symbol\n",
        "    return self.date <= other.date\n",
        "\n",
        "  def __gt__(self, other):\n",
        "    assert self.symbol == other.symbol\n",
        "    return self.date > other.date\n",
        "\n",
        "  def __ge__(self, other):\n",
        "    assert self.symbol == other.symbol\n",
        "    return self.date >= other.date\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    assert self.symbol == other.symbol\n",
        "    return self.date == other.date\n",
        "\n",
        "  def __ne__(self, other):\n",
        "    assert self.symbol == other.symbol\n",
        "    return self.date != other.date\n",
        "\n",
        "  @staticmethod\n",
        "  def create_columns():\n",
        "    return ['Date', 'Symbol', 'Year', 'Month', 'Day', 'Open', 'High', 'Low', 'Close',\n",
        "            'Adjusted Close', 'Volumne', 'Dividend Amount', 'Split Coefficient']\n",
        "\n",
        "  @staticmethod\n",
        "  def parse_from_json(json_object: dict):\n",
        "    try:\n",
        "      open = json_object['1. open']\n",
        "    except Exception:\n",
        "      open = -1\n",
        "\n",
        "    try:\n",
        "      high = json_object['2. high']\n",
        "    except Exception:\n",
        "      high = -1\n",
        "\n",
        "    try:\n",
        "      low = json_object['3. low']\n",
        "    except Exception:\n",
        "      low = -1\n",
        "\n",
        "    try:\n",
        "      close = json_object['4. close']\n",
        "    except Exception:\n",
        "      close = -1\n",
        "        \n",
        "    try:\n",
        "      adjusted_close = json_object['5. adjusted close']\n",
        "    except Exception:\n",
        "      adjusted_close = -1\n",
        "          \n",
        "    try:\n",
        "      volume = json_object['6. volume']\n",
        "    except Exception:\n",
        "      volume = -1\n",
        "\n",
        "    try:\n",
        "      dividend_amount = json_object['7. dividend amount']\n",
        "    except Exception:\n",
        "      dividend_amount = -1\n",
        "\n",
        "    try:\n",
        "      split_coefficient = json_object['8. split coefficient']\n",
        "    except Exception:\n",
        "      split_coefficient = -1\n",
        "    return DailyStock(symbol, date, open, high, low, close,\n",
        "                    adjusted_close, volume, dividend_amount, split_coefficient)\n",
        "    \n",
        "  def create_tuple(self):\n",
        "    return (str(self.date), self.symbol, self.date.year, self.date.month, self.date.day,\n",
        "            self.open, self.high, self.low, self.close, self.adjusted_close,\n",
        "            self.volume, self.dividend_amount, self.split_coefficient)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0iFqCQRClQZ"
      },
      "source": [
        "# Read JSON files and save into .csv files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "symbol_list = []\n",
        "\n",
        "for chunk_number in range(9):\n",
        "  with open('/content/drive/MyDrive/BigData/SymbolList/chunk{}.txt'.format(chunk_number), 'r') as f:\n",
        "    raw_symbol_list = f.readlines()\n",
        "    symbol_list += list(map(lambda x: x.strip('\\n'), raw_symbol_list))\n"
      ],
      "metadata": {
        "id": "d-engbxFkJhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okFc5VYYCrcW"
      },
      "source": [
        "import json\n",
        " \n",
        "symbol = 'AAME'\n",
        "columns = DailyStock.create_columns()\n",
        "df = []\n",
        "f = []\n",
        "for i, symbol in enumerate(symbol_list):\n",
        "  try:\n",
        "    f.append(open(\"/content/drive/MyDrive/BigData/{}.json\".format(symbol), \"r\"))\n",
        "  except Exception:\n",
        "    pass\n",
        "\n",
        "for i, symbol in enumerate(symbol_list[:2]):\n",
        "    data = json.load(f[i])\n",
        "    if 'Time Series (Daily)' not in data:\n",
        "      pass\n",
        "    else:\n",
        "      ds_list = []\n",
        "      for date in data['Time Series (Daily)']:\n",
        "        ds_list.append(DailyStock.parse_from_json(data['Time Series (Daily)'][date]))\n",
        "      \n",
        "      ds_list.sort(key=lambda ds: ds.date, reverse=True)\n",
        "      symbol_df = spark.createDataFrame(list(map(lambda ds: ds.create_tuple(), ds_list)), columns)\n",
        "      symbol_df.toPandas().to_csv('/content/drive/MyDrive/BigDataTest/{}.csv'.format(symbol), index=False)\n",
        "      df.append(symbol_df)\n",
        "      symbol_df.unpersist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLY3ufyvekdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da42940-fd7c-4b26-fa3e-bf581241aa52"
      },
      "source": [
        "import functools\n",
        "\n",
        "df = functools.reduce(lambda a, b: a.union(b), df)\n",
        "df.toPandas().to_csv('/content/drive/MyDrive/BigDataTest/StockData.csv', index=False)\n",
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3484"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}